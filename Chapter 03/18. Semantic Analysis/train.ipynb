{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import FastText\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path, header=None, nrows=3000, skiprows=1, usecols=[1,2,3])\n",
    "    df.rename({1: 'star', 2: 'rating1', 3: 'rating2'}, axis=1, inplace=True)\n",
    "    df['review'] = df['rating1'] + ' ' + df['rating2']\n",
    "    df.drop(columns=['rating1', 'rating2'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_size=0.8):\n",
    "    df_idx = [i for i in range(len(df))]\n",
    "    np.random.shuffle(df_idx)\n",
    "    len_train = int(len(df) * train_size)\n",
    "    df_train = df.iloc[:len_train].reset_index(drop=True)\n",
    "    df_test = df.iloc[len_train:].reset_index(drop=True)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    Get rid of special symbols, lower case the words.\n",
    "    return token list\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_space and not token.is_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_encoder(token, vec):\n",
    "    if token == \"<pad>\":\n",
    "        return 1\n",
    "    else:\n",
    "        try:\n",
    "            return vec.stoi[token]\n",
    "        except:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(tokens, vec):\n",
    "    return [token_encoder(token, vec) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(list_of_idx, max_seq_len, padding_index=1):\n",
    "    output = list_of_idx + (max_seq_len - len(list_of_idx))*[padding_index]\n",
    "    return output[:max_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass(Dataset):\n",
    "    def __init__(self, df, max_seq_len=32):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        train_iter = iter(df.review.values)\n",
    "        self.vec = FastText(\"simple\")\n",
    "        self.vec.vectors[1] = -torch.ones(self.vec.vectors[1].shape[0])\n",
    "        self.vec.vectors[0] = torch.zeros(self.vec.vectors[0].shape[0])\n",
    "        self.vectorizer = lambda x: self.vec.vectors[x]\n",
    "        self.labels = df.star\n",
    "        sequences = [padding(encoder(preprocessing(sequence), self.vec), max_seq_len) for sequence in df.review.tolist()]\n",
    "        self.sequences = sequences\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        assert len(self.sequences[i]) == self.max_seq_len\n",
    "        return self.sequences[i], self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 2, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "df = load_data('/Users/szokirov/Documents/GitHub/strive_practice/Chapter 03/18. Semantic Analysis/.data/Amazon/3000test.csv')\n",
    "df.star = df.star.apply(lambda x: int(x) -1)\n",
    "\n",
    "train_df, test_df = train_test_split(df)\n",
    "train_df, test_df = DataClass(train_df), DataClass(test_df)\n",
    "df.star.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 2, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.star.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train(batch, vectorizer=train_df.vectorizer):\n",
    "    inputs = torch.stack([torch.stack([vectorizer(token) for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch])\n",
    "    return inputs, target\n",
    "\n",
    "def collate_test(batch, vectorizer=test_df.vectorizer):\n",
    "    inputs = torch.stack([torch.stack([vectorizer(token) for token in sentence[0]]) for sentence in batch])\n",
    "    target = torch.LongTensor([item[1] for item in batch])\n",
    "    return inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_df, batch_size=BATCH_SIZE, collate_fn=collate_train, shuffle=True)\n",
    "test_loader = DataLoader(test_df, batch_size=BATCH_SIZE, collate_fn=collate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "emb_dim = 300\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim, hidden1=16, hidden2=16):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(max_seq_len*emb_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 5)\n",
    "        self.out = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs.squeeze(1).float()))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=9600, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=5, bias=True)\n",
       "  (out): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 32\n",
    "model = Classifier(MAX_SEQ_LEN, 300, 16, 16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300\n",
      "\tIteration: 0\t Loss: 0.0163\n",
      "\tIteration: 40\t Loss: 0.6619\n",
      "Epoch: 1/300, Train loss: 0.3177, Test loss: 2.7476, Accuracy: 0.2730\n",
      "Epoch: 2/300\n",
      "\tIteration: 0\t Loss: 0.0115\n",
      "\tIteration: 40\t Loss: 0.6657\n",
      "Epoch: 2/300, Train loss: 0.3023, Test loss: 2.8478, Accuracy: 0.2873\n",
      "Epoch: 3/300\n",
      "\tIteration: 0\t Loss: 0.0174\n",
      "\tIteration: 40\t Loss: 0.6511\n",
      "Epoch: 3/300, Train loss: 0.3001, Test loss: 3.0586, Accuracy: 0.2675\n",
      "Epoch: 4/300\n",
      "\tIteration: 0\t Loss: 0.0152\n",
      "\tIteration: 40\t Loss: 0.6679\n",
      "Epoch: 4/300, Train loss: 0.3001, Test loss: 3.2669, Accuracy: 0.2527\n",
      "Epoch: 5/300\n",
      "\tIteration: 0\t Loss: 0.0171\n",
      "\tIteration: 40\t Loss: 0.6460\n",
      "Epoch: 5/300, Train loss: 0.3000, Test loss: 3.0504, Accuracy: 0.2791\n",
      "Epoch: 6/300\n",
      "\tIteration: 0\t Loss: 0.0216\n",
      "\tIteration: 40\t Loss: 0.6458\n",
      "Epoch: 6/300, Train loss: 0.2955, Test loss: 3.4834, Accuracy: 0.2632\n",
      "Epoch: 7/300\n",
      "\tIteration: 0\t Loss: 0.0174\n",
      "\tIteration: 40\t Loss: 0.6593\n",
      "Epoch: 7/300, Train loss: 0.2895, Test loss: 3.4647, Accuracy: 0.2357\n",
      "Epoch: 8/300\n",
      "\tIteration: 0\t Loss: 0.0145\n",
      "\tIteration: 40\t Loss: 0.6335\n",
      "Epoch: 8/300, Train loss: 0.2912, Test loss: 3.6623, Accuracy: 0.2560\n",
      "Epoch: 9/300\n",
      "\tIteration: 0\t Loss: 0.0097\n",
      "\tIteration: 40\t Loss: 0.6386\n",
      "Epoch: 9/300, Train loss: 0.2921, Test loss: 3.7531, Accuracy: 0.2747\n",
      "Epoch: 10/300\n",
      "\tIteration: 0\t Loss: 0.0207\n",
      "\tIteration: 40\t Loss: 0.6186\n",
      "Epoch: 10/300, Train loss: 0.3058, Test loss: 3.9984, Accuracy: 0.2538\n",
      "Epoch: 11/300\n",
      "\tIteration: 0\t Loss: 0.0154\n",
      "\tIteration: 40\t Loss: 0.6242\n",
      "Epoch: 11/300, Train loss: 0.2988, Test loss: 4.7121, Accuracy: 0.2571\n",
      "Epoch: 12/300\n",
      "\tIteration: 0\t Loss: 0.0168\n",
      "\tIteration: 40\t Loss: 0.6495\n",
      "Epoch: 12/300, Train loss: 0.3072, Test loss: 4.2882, Accuracy: 0.2516\n",
      "Epoch: 13/300\n",
      "\tIteration: 0\t Loss: 0.0121\n",
      "\tIteration: 40\t Loss: 0.6941\n",
      "Epoch: 13/300, Train loss: 0.2965, Test loss: 4.2522, Accuracy: 0.2593\n",
      "Epoch: 14/300\n",
      "\tIteration: 0\t Loss: 0.0170\n",
      "\tIteration: 40\t Loss: 0.6370\n",
      "Epoch: 14/300, Train loss: 0.2633, Test loss: 4.3763, Accuracy: 0.2544\n",
      "Epoch: 15/300\n",
      "\tIteration: 0\t Loss: 0.0125\n",
      "\tIteration: 40\t Loss: 0.5871\n",
      "Epoch: 15/300, Train loss: 0.2769, Test loss: 4.4285, Accuracy: 0.2577\n",
      "Epoch: 16/300\n",
      "\tIteration: 0\t Loss: 0.0193\n",
      "\tIteration: 40\t Loss: 0.6001\n",
      "Epoch: 16/300, Train loss: 0.2784, Test loss: 4.4804, Accuracy: 0.2571\n",
      "Epoch: 17/300\n",
      "\tIteration: 0\t Loss: 0.0126\n",
      "\tIteration: 40\t Loss: 0.5618\n",
      "Epoch: 17/300, Train loss: 0.2746, Test loss: 4.5017, Accuracy: 0.2577\n",
      "Epoch: 18/300\n",
      "\tIteration: 0\t Loss: 0.0142\n",
      "\tIteration: 40\t Loss: 0.5704\n",
      "Epoch: 18/300, Train loss: 0.2710, Test loss: 4.6619, Accuracy: 0.2478\n",
      "Epoch: 19/300\n",
      "\tIteration: 0\t Loss: 0.0120\n",
      "\tIteration: 40\t Loss: 0.5848\n",
      "Epoch: 19/300, Train loss: 0.2536, Test loss: 4.5993, Accuracy: 0.2549\n",
      "Epoch: 20/300\n",
      "\tIteration: 0\t Loss: 0.0244\n",
      "\tIteration: 40\t Loss: 0.5719\n",
      "Epoch: 20/300, Train loss: 0.2585, Test loss: 4.8602, Accuracy: 0.2593\n",
      "Epoch: 21/300\n",
      "\tIteration: 0\t Loss: 0.0140\n",
      "\tIteration: 40\t Loss: 0.5898\n",
      "Epoch: 21/300, Train loss: 0.2557, Test loss: 4.8225, Accuracy: 0.2533\n",
      "Epoch: 22/300\n",
      "\tIteration: 0\t Loss: 0.0131\n",
      "\tIteration: 40\t Loss: 0.5738\n",
      "Epoch: 22/300, Train loss: 0.2620, Test loss: 4.7524, Accuracy: 0.2511\n",
      "Epoch: 23/300\n",
      "\tIteration: 0\t Loss: 0.0164\n",
      "\tIteration: 40\t Loss: 0.5787\n",
      "Epoch: 23/300, Train loss: 0.2563, Test loss: 4.8064, Accuracy: 0.2593\n",
      "Epoch: 24/300\n",
      "\tIteration: 0\t Loss: 0.0153\n",
      "\tIteration: 40\t Loss: 0.5525\n",
      "Epoch: 24/300, Train loss: 0.2665, Test loss: 4.9069, Accuracy: 0.2511\n",
      "Epoch: 25/300\n",
      "\tIteration: 0\t Loss: 0.0139\n",
      "\tIteration: 40\t Loss: 0.5573\n",
      "Epoch: 25/300, Train loss: 0.2498, Test loss: 5.1307, Accuracy: 0.2516\n",
      "Epoch: 26/300\n",
      "\tIteration: 0\t Loss: 0.0106\n",
      "\tIteration: 40\t Loss: 0.5515\n",
      "Epoch: 26/300, Train loss: 0.2755, Test loss: 4.9027, Accuracy: 0.2390\n",
      "Epoch: 27/300\n",
      "\tIteration: 0\t Loss: 0.0174\n",
      "\tIteration: 40\t Loss: 0.5398\n",
      "Epoch: 27/300, Train loss: 0.2513, Test loss: 5.0926, Accuracy: 0.2571\n",
      "Epoch: 28/300\n",
      "\tIteration: 0\t Loss: 0.0110\n",
      "\tIteration: 40\t Loss: 0.5580\n",
      "Epoch: 28/300, Train loss: 0.2472, Test loss: 5.0779, Accuracy: 0.2418\n",
      "Epoch: 29/300\n",
      "\tIteration: 0\t Loss: 0.0140\n",
      "\tIteration: 40\t Loss: 0.5372\n",
      "Epoch: 29/300, Train loss: 0.2457, Test loss: 5.4237, Accuracy: 0.2368\n",
      "Epoch: 30/300\n",
      "\tIteration: 0\t Loss: 0.0128\n",
      "\tIteration: 40\t Loss: 0.5366\n",
      "Epoch: 30/300, Train loss: 0.2333, Test loss: 5.3025, Accuracy: 0.2566\n",
      "Epoch: 31/300\n",
      "\tIteration: 0\t Loss: 0.0095\n",
      "\tIteration: 40\t Loss: 0.5535\n",
      "Epoch: 31/300, Train loss: 0.2337, Test loss: 5.5619, Accuracy: 0.2379\n",
      "Epoch: 32/300\n",
      "\tIteration: 0\t Loss: 0.0152\n",
      "\tIteration: 40\t Loss: 0.5325\n",
      "Epoch: 32/300, Train loss: 0.2420, Test loss: 5.4955, Accuracy: 0.2533\n",
      "Epoch: 33/300\n",
      "\tIteration: 0\t Loss: 0.0229\n",
      "\tIteration: 40\t Loss: 0.5284\n",
      "Epoch: 33/300, Train loss: 0.2357, Test loss: 5.6689, Accuracy: 0.2500\n",
      "Epoch: 34/300\n",
      "\tIteration: 0\t Loss: 0.0172\n",
      "\tIteration: 40\t Loss: 0.6102\n",
      "Epoch: 34/300, Train loss: 0.2389, Test loss: 5.3046, Accuracy: 0.2555\n",
      "Epoch: 35/300\n",
      "\tIteration: 0\t Loss: 0.0141\n",
      "\tIteration: 40\t Loss: 0.5386\n",
      "Epoch: 35/300, Train loss: 0.2526, Test loss: 5.9243, Accuracy: 0.2522\n",
      "Epoch: 36/300\n",
      "\tIteration: 0\t Loss: 0.0229\n",
      "\tIteration: 40\t Loss: 0.7712\n",
      "Epoch: 36/300, Train loss: 0.3365, Test loss: 4.2865, Accuracy: 0.2571\n",
      "Epoch: 37/300\n",
      "\tIteration: 0\t Loss: 0.0296\n",
      "\tIteration: 40\t Loss: 0.6882\n",
      "Epoch: 37/300, Train loss: 0.3038, Test loss: 4.0889, Accuracy: 0.2549\n",
      "Epoch: 38/300\n",
      "\tIteration: 0\t Loss: 0.0164\n",
      "\tIteration: 40\t Loss: 0.5593\n",
      "Epoch: 38/300, Train loss: 0.2566, Test loss: 4.7214, Accuracy: 0.2220\n",
      "Epoch: 39/300\n",
      "\tIteration: 0\t Loss: 0.0133\n",
      "\tIteration: 40\t Loss: 0.5605\n",
      "Epoch: 39/300, Train loss: 0.2438, Test loss: 4.9939, Accuracy: 0.2303\n",
      "Epoch: 40/300\n",
      "\tIteration: 0\t Loss: 0.0112\n",
      "\tIteration: 40\t Loss: 0.5193\n",
      "Epoch: 40/300, Train loss: 0.2460, Test loss: 5.0865, Accuracy: 0.2533\n",
      "Epoch: 41/300\n",
      "\tIteration: 0\t Loss: 0.0097\n",
      "\tIteration: 40\t Loss: 0.5267\n",
      "Epoch: 41/300, Train loss: 0.2445, Test loss: 5.0066, Accuracy: 0.2615\n",
      "Epoch: 42/300\n",
      "\tIteration: 0\t Loss: 0.0115\n",
      "\tIteration: 40\t Loss: 0.5091\n",
      "Epoch: 42/300, Train loss: 0.2363, Test loss: 5.0641, Accuracy: 0.2615\n",
      "Epoch: 43/300\n",
      "\tIteration: 0\t Loss: 0.0149\n",
      "\tIteration: 40\t Loss: 0.5296\n",
      "Epoch: 43/300, Train loss: 0.2223, Test loss: 5.3141, Accuracy: 0.2582\n",
      "Epoch: 44/300\n",
      "\tIteration: 0\t Loss: 0.0203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/szokirov/Documents/GitHub/strive_practice/Chapter 03/18. Semantic Analysis/train.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/szokirov/Documents/GitHub/strive_practice/Chapter%2003/18.%20Semantic%20Analysis/train.ipynb#ch0000017?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(sentences)   \u001b[39m# 1) Forward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/szokirov/Documents/GitHub/strive_practice/Chapter%2003/18.%20Semantic%20Analysis/train.ipynb#ch0000017?line=15'>16</a>\u001b[0m train_loss \u001b[39m=\u001b[39m criterion(output, labels) \u001b[39m# 2) Compute loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/szokirov/Documents/GitHub/strive_practice/Chapter%2003/18.%20Semantic%20Analysis/train.ipynb#ch0000017?line=16'>17</a>\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()                  \u001b[39m# 3) Backward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/szokirov/Documents/GitHub/strive_practice/Chapter%2003/18.%20Semantic%20Analysis/train.ipynb#ch0000017?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()                 \u001b[39m# 4) Update model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/szokirov/Documents/GitHub/strive_practice/Chapter%2003/18.%20Semantic%20Analysis/train.ipynb#ch0000017?line=19'>20</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/szokirov/opt/anaconda3/envs/venv/lib/python3.10/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "print_every = 40\n",
    "train_losses, test_losses, accuracies = [], [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss, running_test_losses, running_test_accuracy = 0, 0, 0\n",
    "    # print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (sentences, labels) in enumerate(iter(train_loader)):\n",
    "\n",
    "        sentences.resize_(sentences.size()[0], 32* emb_dim)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(sentences)   # 1) Forward pass\n",
    "        train_loss = criterion(output, labels) # 2) Compute loss\n",
    "        train_loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += train_loss.item()\n",
    "        \n",
    "        # if i % print_every == 0:\n",
    "        #     print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "        #     running_loss = 0\n",
    "    avg_running_loss = running_loss/len(train_loader)\n",
    "    train_losses.append(avg_running_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (sentences_test, labels_test) in enumerate(iter(test_loader)):\n",
    "            sentences_test.resize_(sentences_test.size()[0], 32* emb_dim)\n",
    "\n",
    "            output_test = model.forward(sentences_test)\n",
    "            test_loss = criterion(output_test, labels_test)\n",
    "\n",
    "            running_test_losses += test_loss.item()\n",
    "\n",
    "            prediction_label = torch.argmax(output_test, dim=1)\n",
    "            running_test_accuracy += (prediction_label == labels_test).sum() / len(labels_test)\n",
    "        avg_test_loss = running_test_losses/len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        avg_running_accuracy = running_test_accuracy/len(test_loader)\n",
    "        accuracies.append(avg_running_accuracy)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{epochs}, Train loss: {avg_running_loss:.4f}, Test loss: {avg_test_loss:.4f}, Accuracy: {avg_running_accuracy:.4f}\" )\n",
    "\n",
    "torch.save({'model_state': model.state_dict()}, 'final_model')\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='Test losses')\n",
    "plt.plot(accuracies, label='Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ec1a62599dbe491b495bbea6ed64b26266314bd43f2a5cdcc432a9a3a8883da"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
